#!/usr/bin/env python3
"""
FMS Consumer for Real Data Processing
ì‹¤ì œ FMS ì„¼ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ì—¬ ì²˜ë¦¬ ë° HDFS ì €ì¥
"""
import json
import os
from confluent_kafka import Consumer, KafkaError
import logging
from datetime import datetime
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import subprocess
from collections import defaultdict
import time
import requests

# Kafka ì„¤ì •
BROKER = "s1:9092,s2:9092,s3:9092"
TOPIC = "fms-sensor-data"
GROUP_ID = "fms-data-processor"

# HDFS ì €ì¥ ê²½ë¡œ
HDFS_BASE_PATH = "/fms/raw"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DEVICE_IDS = [1, 2, 3, 4, 5]

class FMSDataConsumer:
    def __init__(self):
        self.consumer = Consumer({
            'bootstrap.servers': BROKER,
            'group.id': GROUP_ID,
            'auto.offset.reset': 'latest'
        })
        self.batch_buffer = defaultdict(list)
        self.batch_interval = 5  # 5ì´ˆë§ˆë‹¤ ë¬¶ì–´ì„œ ì €ì¥
        self.last_receive_time = time.time()
        self.alert_interval = 30  # 30ì´ˆ ì´ìƒ ìˆ˜ì‹  ì—†ì„ ê²½ìš° ìŠ¬ë™ ì•Œë¦¼
        self.last_alert_sent = 0  # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€

    def validate_data(self, data):
        """ë°ì´í„° í•„ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬"""
        required_fields = [
            'time', 'DeviceId'
        ]

        # 1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€
        if not all(field in data for field in required_fields):
            logger.warning("âš ï¸ ëˆ„ë½ëœ í•„ë“œ ìˆìŒ â†’ ê±´ë„ˆëœ€")
            return False

        # 2. time â†’ ISO8601 í˜•ì‹ ê²€ì‚¬
        try:
            datetime.fromisoformat(data['time'])
        except Exception:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ timestamp í˜•ì‹: {data['time']}")
            return False

        # # 3. DeviceId ê°’ í˜•ë³€í™˜ ê°€ëŠ¥ ì—¬ë¶€
        try:
            data['DeviceId'] = int(data['DeviceId'])
        except Exception:
            logger.warning(f"âš ï¸ DeviceId í˜•ë³€í™˜ ì‹¤íŒ¨: {data['DeviceId']}")
            return False

        # 4. ì„¼ì„œ ê°’ í˜•ë³€í™˜ ê°€ëŠ¥ ì—¬ë¶€
        for key in ['sensor1', 'sensor2', 'sensor3']:
            try:
                data[key] = float(data[key])
            except Exception:
                logger.warning(f"âš ï¸ {key} ê°’ í˜•ë³€í™˜ ì‹¤íŒ¨: {data[key]}")
                return False

        # 5. ëª¨í„° ê°’ í˜•ë³€í™˜ ê°€ëŠ¥ ì—¬ë¶€
        for key in ['motor1', 'motor2', 'motor3']:
            try:
                data[key] = int(data[key])
            except Exception:
                logger.warning(f"âš ï¸ {key} ê°’ í˜•ë³€í™˜ ì‹¤íŒ¨: {data[key]}")
                return False

        return True

    def run(self):
        logger.info("FMS Data Consumer ì‹œì‘...")
        logger.info(f"êµ¬ë… í† í”½: {TOPIC}")

        self.consumer.subscribe([TOPIC])

        last_flush_time = time.time()

        try:
            while True:
                msgs = self.consumer.consume(num_messages=100, timeout=1.0)
                if msgs:
                    for msg in msgs:
                        if msg is None:
                            continue
                        elif msg.error():
                            if msg.error().code() != KafkaError._PARTITION_EOF:
                                logger.error(f"Consumer error: {msg.error()}")
                                break
                        else:
                            data = json.loads(msg.value().decode('utf-8'))
                            self.last_receive_time = time.time()
                            if self.validate_data(data):
                                ts_key = datetime.fromisoformat(data['time']).strftime('%Y%m%d%H%M%S')
                                self.batch_buffer[ts_key].append(data)

                now = time.time()
                if now - last_flush_time >= self.batch_interval:
                    self.flush_batches()
                    last_flush_time = now

                # Kafkaë¡œë¶€í„° ë„ˆë¬´ ì˜¤ë«ë™ì•ˆ ë©”ì‹œì§€ë¥¼ ëª» ë°›ì•˜ì„ ê²½ìš° Slack ì•Œë¦¼
                now = time.time()
                if now - self.last_receive_time > self.alert_interval:
                    if now - self.last_alert_sent > self.alert_interval:
                        self.send_slack_alert("âš ï¸ FMS Kafka í† í”½ì—ì„œ 30ì´ˆ ì´ìƒ ë°ì´í„° ìˆ˜ì‹  ì—†ìŒ")
                        self.last_alert_sent = now

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        finally:
            self.consumer.close()
            logger.info("Consumer ì¢…ë£Œ")

    def flush_batches(self):
        try:
            for ts_key, records in self.batch_buffer.items():
                if records:
                    ts = datetime.fromisoformat(records[0]['time'])
                    hdfs_path = f"{HDFS_BASE_PATH}/year={ts.year}/month={ts.month:02d}/day={ts.day:02d}/hour={ts.hour:02d}/minute={ts.minute:02d}"
                    now_str = datetime.now().strftime("%Y%m%d%H%M%S")
                    file_name = f"{ts_key}_{now_str}.parquet"
                    local_tmp_file = f"/tmp/{file_name}"

                    df = pd.DataFrame(records)
                    table = pa.Table.from_pandas(df)
                    pq.write_table(table, local_tmp_file, compression='snappy')

                    subprocess.run(["hdfs", "dfs", "-mkdir", "-p", hdfs_path], check=True)
                    subprocess.run(["hdfs", "dfs", "-put", "-f", local_tmp_file, f"{hdfs_path}/{file_name}"], check=True)
                    os.remove(local_tmp_file)

                    logger.info(f"âœ… {len(records)}ê°œ â†’ HDFS ì €ì¥ ì™„ë£Œ: {hdfs_path}/{file_name}")

            self.batch_buffer.clear()
        except Exception as e:
            logger.error(f"HDFS ì €ì¥ ì‹¤íŒ¨: {e}")

    def send_slack_alert(self, message):
        webhook_url = os.environ.get("SLACK_WEBHOOK_URL")
        if not webhook_url:
            logger.warning("Slack Webhook URLì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŒ")
            return
        payload = {
            "text": message
        }
        try:
            requests.post(webhook_url, json=payload)
            logger.info("ğŸ“¢ Slack ì•Œë¦¼ ì „ì†¡ë¨")
        except Exception as e:
            logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    consumer = FMSDataConsumer()
    consumer.run()